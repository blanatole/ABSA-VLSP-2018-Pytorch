#!/usr/bin/env python3
"""
Demo script Ä‘á»ƒ xuáº¥t káº¿t quáº£ tá»«ng bÆ°á»›c tiá»n xá»­ lÃ½ vÄƒn báº£n tiáº¿ng Viá»‡t
Hiá»ƒn thá»‹ chi tiáº¿t quÃ¡ trÃ¬nh xá»­ lÃ½ text tá»« raw input Ä‘áº¿n final output
"""

import os
import sys
import re
import emoji
from pathlib import Path
from typing import List, Dict
import pandas as pd
from datetime import datetime

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

try:
    from data_processing import (
        VietnameseTextCleaner, 
        VietnameseToneNormalizer, 
        VietnameseTextPreprocessor
    )
except ImportError:
    print("Warning: Could not import from src/data_processing.py")
    print("Using simplified preprocessing classes...")
    
    class SimpleVietnameseTextCleaner:
        VN_CHARS = 'Ã¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘ÃÃ€áº¢Ãƒáº Ä‚áº®áº°áº²áº´áº¶Ã‚áº¤áº¦áº¨áºªáº¬Ã‰Ãˆáººáº¼áº¸ÃŠáº¾á»€á»‚á»„á»†Ã“Ã’á»Ã•á»ŒÃ”á»á»’á»”á»–á»˜Æ á»šá»œá»á» á»¢ÃÃŒá»ˆÄ¨á»ŠÃšÃ™á»¦Å¨á»¤Æ¯á»¨á»ªá»¬á»®á»°Ãá»²á»¶á»¸á»´Ä'
        
        @staticmethod
        def remove_html(text: str) -> str:
            return re.sub(r'<[^>]*>', '', text)
        
        @staticmethod
        def remove_emoji(text: str) -> str:
            try:
                return emoji.replace_emoji(text, '')
            except:
                return re.sub(r'[ğŸ˜€-ğŸ™]', '', text)
        
        @staticmethod
        def remove_url(text: str) -> str:
            return re.sub(r'https?:\/\/(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()!@:%_\+.~#?&\/\/=]*)', '', text)
        
        @staticmethod
        def remove_email(text: str) -> str:
            return re.sub(r'[^@ \t\r\n]+@[^@ \t\r\n]+\.[^@ \t\r\n]+', '', text)
        
        @staticmethod
        def remove_phone_number(text: str) -> str:
            return re.sub(r'^[\+]?[(]?[0-9]{3}[)]?[-\s\.]?[0-9]{3}[-\s\.]?[0-9]{4,6}$', '', text)
        
        @staticmethod
        def remove_hashtags(text: str) -> str:
            return re.sub(r'#\w+', '', text)
        
        @staticmethod
        def remove_unnecessary_characters(text: str) -> str:
            text = re.sub(fr"[^\sa-zA-Z0-9{SimpleVietnameseTextCleaner.VN_CHARS}]", ' ', text)
            return re.sub(r'\s+', ' ', text).strip()
        
        @staticmethod
        def process_text(text: str) -> str:
            """Apply all cleaning steps"""
            text = SimpleVietnameseTextCleaner.remove_html(text)
            text = SimpleVietnameseTextCleaner.remove_emoji(text)
            text = SimpleVietnameseTextCleaner.remove_url(text)
            text = SimpleVietnameseTextCleaner.remove_email(text)
            text = SimpleVietnameseTextCleaner.remove_phone_number(text)
            text = SimpleVietnameseTextCleaner.remove_hashtags(text)
            text = SimpleVietnameseTextCleaner.remove_unnecessary_characters(text)
            return text

    VietnameseTextCleaner = SimpleVietnameseTextCleaner


def print_step_header(step_num: int, step_name: str, description: str = ""):
    """In header cho má»—i bÆ°á»›c"""
    print(f"\n{'='*80}")
    print(f"BÆ¯á»šC {step_num}: {step_name}")
    if description:
        print(f"ğŸ“ {description}")
    print('='*80)


def print_step_result(original: str, processed: str, step_name: str):
    """In káº¿t quáº£ cá»§a tá»«ng bÆ°á»›c xá»­ lÃ½"""
    print(f"\nğŸ”¸ {step_name}:")
    print(f"   TrÆ°á»›c: '{original}'")
    print(f"   Sau:   '{processed}'")
    
    if original != processed:
        print(f"   âœ… CÃ³ thay Ä‘á»•i")
    else:
        print(f"   â¡ï¸  KhÃ´ng thay Ä‘á»•i")


def normalize_teencodes_simple(text: str) -> str:
    """Normalize teencode Ä‘Æ¡n giáº£n"""
    teencodes = {
        'ks': 'khÃ¡ch sáº¡n',
        'nhahang': 'nhÃ  hÃ ng', 
        'nv': 'nhÃ¢n viÃªn',
        'dv': 'dá»‹ch vá»¥',
        'pt': 'phÃ²ng táº¯m',
        'dt': 'Ä‘iá»‡n thoáº¡i',
        'fb': 'facebook',
        'ok': 'Ä‘Æ°á»£c',
        'ko': 'khÃ´ng',
        'k': 'khÃ´ng',
        'tks': 'cáº£m Æ¡n',
        'thanks': 'cáº£m Æ¡n',
        'gud': 'tá»‘t',
        'good': 'tá»‘t',
        'bad': 'tá»‡',
        'wa': 'quÃ¡',
        'vs': 'vá»›i',
        'j': 'gÃ¬',
        'r': 'rá»“i',
        'dc': 'Ä‘Æ°á»£c',
        'Ä‘c': 'Ä‘Æ°á»£c'
    }
    
    words = text.split()
    processed_words = []
    
    for word in words:
        if word.lower() in teencodes:
            processed_words.append(teencodes[word.lower()])
        else:
            processed_words.append(word)
    
    return ' '.join(processed_words)


def normalize_tone_simple(text: str) -> str:
    """Normalize tone Ä‘Æ¡n giáº£n - fix má»™t sá»‘ lá»—i thÆ°á»ng gáº·p"""
    # Má»™t sá»‘ normalization cÆ¡ báº£n cho tiáº¿ng Viá»‡t
    tone_fixes = {
        'lá»±Æ¡ng': 'lÆ°á»£ng',
        'thá»ai': 'thoáº£i', 
        'hoÃ ': 'hÃ²a',
        'khoÃ¡': 'khÃ³a',
        'toÃ ': 'tÃ²a',
        'gia Ä‘Ã¬nh': 'gia Ä‘Ã¬nh',  # giá»¯ nguyÃªn
    }
    
    for wrong, correct in tone_fixes.items():
        text = text.replace(wrong, correct)
    
    return text


def segment_words_simple(text: str) -> str:
    """Word segmentation Ä‘Æ¡n giáº£n - chá»‰ handle má»™t sá»‘ trÆ°á»ng há»£p cÆ¡ báº£n"""
    # Má»™t sá»‘ compound words tiáº¿ng Viá»‡t cáº§n segment
    compound_words = {
        'khÃ¡ch sáº¡n': 'khÃ¡ch_sáº¡n',
        'nhÃ  hÃ ng': 'nhÃ _hÃ ng',
        'nhÃ¢n viÃªn': 'nhÃ¢n_viÃªn',
        'dá»‹ch vá»¥': 'dá»‹ch_vá»¥',
        'phÃ²ng táº¯m': 'phÃ²ng_táº¯m',
        'Ä‘iá»‡n thoáº¡i': 'Ä‘iá»‡n_thoáº¡i',
        'cÃ¡m Æ¡n': 'cÃ¡m_Æ¡n',
        'cáº£m Æ¡n': 'cáº£m_Æ¡n',
        'khÃ´ng gian': 'khÃ´ng_gian',
        'thá»i gian': 'thá»i_gian',
        'cháº¥t lÆ°á»£ng': 'cháº¥t_lÆ°á»£ng',
        'giÃ¡ cáº£': 'giÃ¡_cáº£',
        'vá»‹ trÃ­': 'vá»‹_trÃ­',
        'mÃ¡y láº¡nh': 'mÃ¡y_láº¡nh'
    }
    
    for phrase, segmented in compound_words.items():
        text = text.replace(phrase, segmented)
    
    return text


def demo_preprocessing_pipeline(sample_texts: List[str], output_file: str = None):
    """Demo complete preprocessing pipeline vá»›i output chi tiáº¿t"""
    
    print("ğŸš€ DEMO PIPELINE TIá»€N Xá»¬ LÃ VÄ‚N Báº¢N TIáº¾NG VIá»†T")
    print("ğŸ“… Thá»i gian:", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("\nğŸ“‹ Pipeline gá»“m cÃ¡c bÆ°á»›c:")
    print("   1. Chuyá»ƒn vá» chá»¯ thÆ°á»ng (Lowercase)")
    print("   2. Loáº¡i bá» HTML tags")
    print("   3. Loáº¡i bá» emoji")
    print("   4. Loáº¡i bá» URL") 
    print("   5. Loáº¡i bá» email")
    print("   6. Loáº¡i bá» sá»‘ Ä‘iá»‡n thoáº¡i")
    print("   7. Loáº¡i bá» hashtags")
    print("   8. Loáº¡i bá» kÃ½ tá»± khÃ´ng cáº§n thiáº¿t")
    print("   9. Chuáº©n hÃ³a tone tiáº¿ng Viá»‡t")
    print("  10. Chuáº©n hÃ³a teencode")
    print("  11. PhÃ¢n Ä‘oáº¡n tá»« (Word Segmentation)")
    
    results = []
    
    for idx, original_text in enumerate(sample_texts):
        print(f"\n\n{'ğŸ¯'*3} SAMPLE {idx + 1} {'ğŸ¯'*3}")
        print(f"ğŸ“ Text gá»‘c: '{original_text}'")
        
        # BÆ°á»›c 1: Lowercase
        current_text = original_text
        step1_text = current_text.lower()
        print_step_header(1, "CHUYá»‚N CHá»® THÆ¯á»œNG", "ÄÆ°a táº¥t cáº£ vá» lowercase Ä‘á»ƒ chuáº©n hÃ³a")
        print_step_result(current_text, step1_text, "Lowercase")
        current_text = step1_text
        
        # BÆ°á»›c 2: Remove HTML
        step2_text = VietnameseTextCleaner.remove_html(current_text)
        print_step_header(2, "LOáº I Bá» HTML TAGS", "XÃ³a cÃ¡c tag HTML nhÆ° <b>, <div>, etc.")
        print_step_result(current_text, step2_text, "Remove HTML")
        current_text = step2_text
        
        # BÆ°á»›c 3: Remove Emoji
        step3_text = VietnameseTextCleaner.remove_emoji(current_text)
        print_step_header(3, "LOáº I Bá» EMOJI", "XÃ³a cÃ¡c emoji vÃ  emoticon")
        print_step_result(current_text, step3_text, "Remove Emoji")
        current_text = step3_text
        
        # BÆ°á»›c 4: Remove URL
        step4_text = VietnameseTextCleaner.remove_url(current_text)
        print_step_header(4, "LOáº I Bá» URL", "XÃ³a cÃ¡c liÃªn káº¿t web")
        print_step_result(current_text, step4_text, "Remove URL")
        current_text = step4_text
        
        # BÆ°á»›c 5: Remove Email
        step5_text = VietnameseTextCleaner.remove_email(current_text)
        print_step_header(5, "LOáº I Bá» EMAIL", "XÃ³a Ä‘á»‹a chá»‰ email")
        print_step_result(current_text, step5_text, "Remove Email")
        current_text = step5_text
        
        # BÆ°á»›c 6: Remove Phone
        step6_text = VietnameseTextCleaner.remove_phone_number(current_text)
        print_step_header(6, "LOáº I Bá» Sá» ÄIá»†N THOáº I", "XÃ³a sá»‘ Ä‘iá»‡n thoáº¡i")
        print_step_result(current_text, step6_text, "Remove Phone")
        current_text = step6_text
        
        # BÆ°á»›c 7: Remove Hashtags
        step7_text = VietnameseTextCleaner.remove_hashtags(current_text)
        print_step_header(7, "LOáº I Bá» HASHTAGS", "XÃ³a cÃ¡c hashtag #tag")
        print_step_result(current_text, step7_text, "Remove Hashtags")
        current_text = step7_text
        
        # BÆ°á»›c 8: Remove Unnecessary Characters
        step8_text = VietnameseTextCleaner.remove_unnecessary_characters(current_text)
        print_step_header(8, "LOáº I Bá» KÃ Tá»° KHÃ”NG Cáº¦N THIáº¾T", "Chá»‰ giá»¯ chá»¯, sá»‘ vÃ  dáº¥u tiáº¿ng Viá»‡t")
        print_step_result(current_text, step8_text, "Clean Characters")
        current_text = step8_text
        
        # BÆ°á»›c 9: Normalize Tone
        step9_text = normalize_tone_simple(current_text)
        print_step_header(9, "CHUáº¨N HÃ“A TONE TIáº¾NG VIá»†T", "Sá»­a lá»—i gÃµ tone nhÆ° lá»±Æ¡ng -> lÆ°á»£ng")
        print_step_result(current_text, step9_text, "Normalize Tone")
        current_text = step9_text
        
        # BÆ°á»›c 10: Normalize Teencodes
        step10_text = normalize_teencodes_simple(current_text)
        print_step_header(10, "CHUáº¨N HÃ“A TEENCODE", "Chuyá»ƒn teencode thÃ nh tá»« chuáº©n: ks -> khÃ¡ch sáº¡n")
        print_step_result(current_text, step10_text, "Normalize Teencode")
        current_text = step10_text
        
        # BÆ°á»›c 11: Word Segmentation
        step11_text = segment_words_simple(current_text)
        print_step_header(11, "PHÃ‚N ÄOáº N Tá»ª", "GhÃ©p tá»« ghÃ©p: khÃ¡ch sáº¡n -> khÃ¡ch_sáº¡n")
        print_step_result(current_text, step11_text, "Word Segmentation")
        final_text = step11_text
        
        # Tá»•ng káº¿t
        print(f"\n{'ğŸ‰'*10} Káº¾T QUáº¢ CUá»I CÃ™NG {'ğŸ‰'*10}")
        print(f"ğŸ“¥ Input:  '{original_text}'")
        print(f"ğŸ“¤ Output: '{final_text}'")
        
        # Thá»‘ng kÃª thay Ä‘á»•i
        words_before = len(original_text.split())
        words_after = len(final_text.split())
        chars_before = len(original_text)
        chars_after = len(final_text)
        
        print(f"\nğŸ“Š Thá»‘ng kÃª:")
        print(f"   â€¢ Sá»‘ tá»«: {words_before} â†’ {words_after} ({'+'*(words_after-words_before) if words_after >= words_before else str(words_after-words_before)})")
        print(f"   â€¢ Sá»‘ kÃ½ tá»±: {chars_before} â†’ {chars_after} ({'+'*(chars_after-chars_before) if chars_after >= chars_before else str(chars_after-chars_before)})")
        
        results.append({
            'sample_id': idx + 1,
            'original_text': original_text,
            'step1_lowercase': step1_text,
            'step2_remove_html': step2_text,
            'step3_remove_emoji': step3_text,
            'step4_remove_url': step4_text,
            'step5_remove_email': step5_text,
            'step6_remove_phone': step6_text,
            'step7_remove_hashtags': step7_text,
            'step8_clean_chars': step8_text,
            'step9_normalize_tone': step9_text,
            'step10_normalize_teencode': step10_text,
            'step11_word_segment': step11_text,
            'final_output': final_text,
            'words_before': words_before,
            'words_after': words_after,
            'chars_before': chars_before,
            'chars_after': chars_after
        })
    
    # Xuáº¥t káº¿t quáº£ ra file CSV náº¿u Ä‘Æ°á»£c yÃªu cáº§u
    if output_file:
        df = pd.DataFrame(results)
        df.to_csv(output_file, index=False, encoding='utf-8')
        print(f"\nğŸ’¾ ÄÃ£ lÆ°u káº¿t quáº£ chi tiáº¿t vÃ o: {output_file}")
    
    return results


def main():
    """Main function"""
    print("ğŸ‡»ğŸ‡³ DEMO TIá»€N Xá»¬ LÃ VÄ‚N Báº¢N TIáº¾NG VIá»†T - ABSA VLSP 2018")
    print("=" * 80)
    
    # Sample texts cho demo
    sample_texts = [
        # Text vá»›i HTML vÃ  emoji
        "Ks nÃ y ráº¥t <b>Ä‘áº¹p</b> vÃ  sáº¡ch sáº½ ğŸ˜! Nv thÃ¢n thiá»‡n ok.",
        
        # Text vá»›i URL vÃ  email
        "Nhahang tá»‘t, xem thÃªm táº¡i https://example.com hoáº·c liÃªn há»‡ test@gmail.com",
        
        # Text vá»›i hashtags vÃ  phone
        "MÃ³n Äƒn ngon wa #food #delicious. Gá»i 0901234567 Ä‘á»ƒ Ä‘áº·t bÃ n.",
        
        # Text vá»›i lá»—i tone vÃ  teencode
        "Cháº¥t lá»±Æ¡ng ko tá»‘t, thá»ai mÃ¡i vs giÃ¡ cáº£ há»£p lÃ½. Tks!",
        
        # Text phá»©c táº¡p vá»›i nhiá»u váº¥n Ä‘á»
        "CÃ¡m Æ¡n shop ğŸ˜Š! Sáº£n pháº©m gud, ship nhanh. Visit: www.shop.com #shopping ğŸ‘",
        
        # Text thá»±c táº¿ tá»« review khÃ¡ch sáº¡n
        "PhÃ²ng sáº¡ch sáº½, nhÃ¢n viÃªn dv tá»‘t. Vá»‹ trÃ­ khÃ¡ch sáº¡n thuáº­n tiá»‡n, gáº§n trung tÃ¢m.",
        
        # Text vá»›i nhiá»u kÃ½ tá»± Ä‘áº·c biá»‡t
        "!!! GiÃ¡ cáº£ ok, cháº¥t lÆ°á»£ng @#$% khÃ´ng nhÆ° mong Ä‘á»£i !!! FB: hotelpage"
    ]
    
    # Cháº¡y demo
    results = demo_preprocessing_pipeline(
        sample_texts, 
        output_file='preprocessing_demo_results.csv'
    )
    
    print(f"\n\n{'ğŸ”¥'*20} Tá»”NG Káº¾T {'ğŸ”¥'*20}")
    print(f"âœ… ÄÃ£ xá»­ lÃ½ {len(sample_texts)} sample texts")
    print(f"ğŸ“Š Káº¿t quáº£ tá»•ng thá»ƒ:")
    
    total_chars_before = sum(r['chars_before'] for r in results)
    total_chars_after = sum(r['chars_after'] for r in results)
    total_words_before = sum(r['words_before'] for r in results)
    total_words_after = sum(r['words_after'] for r in results)
    
    print(f"   â€¢ Tá»•ng kÃ½ tá»±: {total_chars_before} â†’ {total_chars_after}")
    print(f"   â€¢ Tá»•ng tá»«: {total_words_before} â†’ {total_words_after}")
    print(f"   â€¢ Tá»· lá»‡ nÃ©n kÃ½ tá»±: {total_chars_after/total_chars_before:.2%}")
    print(f"   â€¢ Tá»· lá»‡ thay Ä‘á»•i tá»«: {total_words_after/total_words_before:.2%}")
    
    print(f"\nğŸ¯ CÃ¡c bÆ°á»›c quan trá»ng nháº¥t:")
    print(f"   1. Chuáº©n hÃ³a teencode (ks â†’ khÃ¡ch sáº¡n)")
    print(f"   2. Loáº¡i bá» kÃ½ tá»± khÃ´ng cáº§n thiáº¿t")
    print(f"   3. PhÃ¢n Ä‘oáº¡n tá»« (khÃ¡ch sáº¡n â†’ khÃ¡ch_sáº¡n)")
    print(f"   4. Chuáº©n hÃ³a tone tiáº¿ng Viá»‡t")
    
    print(f"\nğŸ“ LÆ°u Ã½: Script nÃ y sá»­ dá»¥ng preprocessing Ä‘Æ¡n giáº£n.")
    print(f"   Trong thá»±c táº¿, project sá»­ dá»¥ng:")
    print(f"   â€¢ VnCoreNLP cho word segmentation chÃ­nh xÃ¡c")
    print(f"   â€¢ Model bmd1905/vietnamese-correction-v2 cho error correction")
    print(f"   â€¢ Teencode dictionary Ä‘áº§y Ä‘á»§ tá»« behitek")


if __name__ == "__main__":
    main() 