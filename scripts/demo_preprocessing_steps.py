#!/usr/bin/env python3
"""
Demo script ƒë·ªÉ xu·∫•t k·∫øt qu·∫£ t·ª´ng b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n ti·∫øng Vi·ªát
Hi·ªÉn th·ªã chi ti·∫øt qu√° tr√¨nh x·ª≠ l√Ω text t·ª´ raw input ƒë·∫øn final output
"""

import os
import sys
import re
import emoji
from pathlib import Path
from typing import List, Dict
import pandas as pd
from datetime import datetime

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

try:
    from data_processing import (
        VietnameseTextCleaner, 
        VietnameseToneNormalizer, 
        VietnameseTextPreprocessor
    )
except ImportError:
    print("Warning: Could not import from src/data_processing.py")
    print("Using simplified preprocessing classes...")
    
    class SimpleVietnameseTextCleaner:
        VN_CHARS = '√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√≠√¨·ªâƒ©·ªã√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµƒë√Å√Ä·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√â√à·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√ì√í·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ç√å·ªàƒ®·ªä√ö√ô·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞√ù·ª≤·ª∂·ª∏·ª¥ƒê'
        
        @staticmethod
        def remove_html(text: str) -> str:
            return re.sub(r'<[^>]*>', '', text)
        
        @staticmethod
        def remove_emoji(text: str) -> str:
            try:
                return emoji.replace_emoji(text, '')
            except:
                return re.sub(r'[üòÄ-üôè]', '', text)
        
        @staticmethod
        def remove_url(text: str) -> str:
            return re.sub(r'https?:\/\/(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()!@:%_\+.~#?&\/\/=]*)', '', text)
        
        @staticmethod
        def remove_email(text: str) -> str:
            return re.sub(r'[^@ \t\r\n]+@[^@ \t\r\n]+\.[^@ \t\r\n]+', '', text)
        
        @staticmethod
        def remove_phone_number(text: str) -> str:
            return re.sub(r'^[\+]?[(]?[0-9]{3}[)]?[-\s\.]?[0-9]{3}[-\s\.]?[0-9]{4,6}$', '', text)
        
        @staticmethod
        def remove_hashtags(text: str) -> str:
            return re.sub(r'#\w+', '', text)
        
        @staticmethod
        def remove_unnecessary_characters(text: str) -> str:
            text = re.sub(fr"[^\sa-zA-Z0-9{SimpleVietnameseTextCleaner.VN_CHARS}]", ' ', text)
            return re.sub(r'\s+', ' ', text).strip()
        
        @staticmethod
        def process_text(text: str) -> str:
            """Apply all cleaning steps"""
            text = SimpleVietnameseTextCleaner.remove_html(text)
            text = SimpleVietnameseTextCleaner.remove_emoji(text)
            text = SimpleVietnameseTextCleaner.remove_url(text)
            text = SimpleVietnameseTextCleaner.remove_email(text)
            text = SimpleVietnameseTextCleaner.remove_phone_number(text)
            text = SimpleVietnameseTextCleaner.remove_hashtags(text)
            text = SimpleVietnameseTextCleaner.remove_unnecessary_characters(text)
            return text

    VietnameseTextCleaner = SimpleVietnameseTextCleaner


def print_step_header(step_num: int, step_name: str, description: str = ""):
    """In header cho m·ªói b∆∞·ªõc"""
    print(f"\n{'='*80}")
    print(f"B∆Ø·ªöC {step_num}: {step_name}")
    if description:
        print(f"üìù {description}")
    print('='*80)


def print_step_result(original: str, processed: str, step_name: str):
    """In k·∫øt qu·∫£ c·ªßa t·ª´ng b∆∞·ªõc x·ª≠ l√Ω"""
    print(f"\nüî∏ {step_name}:")
    print(f"   Tr∆∞·ªõc: '{original}'")
    print(f"   Sau:   '{processed}'")
    
    if original != processed:
        print(f"   ‚úÖ C√≥ thay ƒë·ªïi")
    else:
        print(f"   ‚û°Ô∏è  Kh√¥ng thay ƒë·ªïi")


def normalize_teencodes_simple(text: str) -> str:
    """Normalize teencode ƒë∆°n gi·∫£n"""
    teencodes = {
        'ks': 'kh√°ch s·∫°n',
        'nhahang': 'nh√† h√†ng', 
        'nv': 'nh√¢n vi√™n',
        'dv': 'd·ªãch v·ª•',
        'pt': 'ph√≤ng t·∫Øm',
        'dt': 'ƒëi·ªán tho·∫°i',
        'fb': 'facebook',
        'ok': 'ƒë∆∞·ª£c',
        'ko': 'kh√¥ng',
        'k': 'kh√¥ng',
        'tks': 'c·∫£m ∆°n',
        'thanks': 'c·∫£m ∆°n',
        'gud': 't·ªët',
        'good': 't·ªët',
        'bad': 't·ªá',
        'wa': 'qu√°',
        'vs': 'v·ªõi',
        'j': 'g√¨',
        'r': 'r·ªìi',
        'dc': 'ƒë∆∞·ª£c',
        'ƒëc': 'ƒë∆∞·ª£c'
    }
    
    words = text.split()
    processed_words = []
    
    for word in words:
        if word.lower() in teencodes:
            processed_words.append(teencodes[word.lower()])
        else:
            processed_words.append(word)
    
    return ' '.join(processed_words)


def normalize_tone_simple(text: str) -> str:
    """Normalize tone ƒë∆°n gi·∫£n - fix m·ªôt s·ªë l·ªói th∆∞·ªùng g·∫∑p"""
    # M·ªôt s·ªë normalization c∆° b·∫£n cho ti·∫øng Vi·ªát
    tone_fixes = {
        'l·ª±∆°ng': 'l∆∞·ª£ng',
        'th·ªèai': 'tho·∫£i', 
        'ho√†': 'h√≤a',
        'kho√°': 'kh√≥a',
        'to√†': 't√≤a',
        'gia ƒë√¨nh': 'gia ƒë√¨nh',  # gi·ªØ nguy√™n
    }
    
    for wrong, correct in tone_fixes.items():
        text = text.replace(wrong, correct)
    
    return text


def segment_words_simple(text: str) -> str:
    """Word segmentation ƒë∆°n gi·∫£n - ch·ªâ handle m·ªôt s·ªë tr∆∞·ªùng h·ª£p c∆° b·∫£n"""
    # M·ªôt s·ªë compound words ti·∫øng Vi·ªát c·∫ßn segment
    compound_words = {
        'kh√°ch s·∫°n': 'kh√°ch_s·∫°n',
        'nh√† h√†ng': 'nh√†_h√†ng',
        'nh√¢n vi√™n': 'nh√¢n_vi√™n',
        'd·ªãch v·ª•': 'd·ªãch_v·ª•',
        'ph√≤ng t·∫Øm': 'ph√≤ng_t·∫Øm',
        'ƒëi·ªán tho·∫°i': 'ƒëi·ªán_tho·∫°i',
        'c√°m ∆°n': 'c√°m_∆°n',
        'c·∫£m ∆°n': 'c·∫£m_∆°n',
        'kh√¥ng gian': 'kh√¥ng_gian',
        'th·ªùi gian': 'th·ªùi_gian',
        'ch·∫•t l∆∞·ª£ng': 'ch·∫•t_l∆∞·ª£ng',
        'gi√° c·∫£': 'gi√°_c·∫£',
        'v·ªã tr√≠': 'v·ªã_tr√≠',
        'm√°y l·∫°nh': 'm√°y_l·∫°nh'
    }
    
    for phrase, segmented in compound_words.items():
        text = text.replace(phrase, segmented)
    
    return text


def demo_preprocessing_pipeline(sample_texts: List[str], output_file: str = None):
    """Demo complete preprocessing pipeline v·ªõi output chi ti·∫øt"""
    
    print("üöÄ DEMO PIPELINE TI·ªÄN X·ª¨ L√ù VƒÇN B·∫¢N TI·∫æNG VI·ªÜT")
    print("üìÖ Th·ªùi gian:", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("\nüìã Pipeline g·ªìm c√°c b∆∞·ªõc:")
    print("   1. Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng (Lowercase)")
    print("   2. Lo·∫°i b·ªè HTML tags")
    print("   3. Lo·∫°i b·ªè emoji")
    print("   4. Lo·∫°i b·ªè URL") 
    print("   5. Lo·∫°i b·ªè email")
    print("   6. Lo·∫°i b·ªè s·ªë ƒëi·ªán tho·∫°i")
    print("   7. Lo·∫°i b·ªè hashtags")
    print("   8. Lo·∫°i b·ªè k√Ω t·ª± kh√¥ng c·∫ßn thi·∫øt")
    print("   9. Chu·∫©n h√≥a tone ti·∫øng Vi·ªát")
    print("  10. Chu·∫©n h√≥a teencode")
    print("  11. Ph√¢n ƒëo·∫°n t·ª´ (Word Segmentation)")
    
    results = []
    
    for idx, original_text in enumerate(sample_texts):
        print(f"\n\n{'üéØ'*3} SAMPLE {idx + 1} {'üéØ'*3}")
        print(f"üìù Text g·ªëc: '{original_text}'")
        
        # B∆∞·ªõc 1: Lowercase
        current_text = original_text
        step1_text = current_text.lower()
        print_step_header(1, "CHUY·ªÇN CH·ªÆ TH∆Ø·ªúNG", "ƒê∆∞a t·∫•t c·∫£ v·ªÅ lowercase ƒë·ªÉ chu·∫©n h√≥a")
        print_step_result(current_text, step1_text, "Lowercase")
        current_text = step1_text
        
        # B∆∞·ªõc 2: Remove HTML
        step2_text = VietnameseTextCleaner.remove_html(current_text)
        print_step_header(2, "LO·∫†I B·ªé HTML TAGS", "X√≥a c√°c tag HTML nh∆∞ <b>, <div>, etc.")
        print_step_result(current_text, step2_text, "Remove HTML")
        current_text = step2_text
        
        # B∆∞·ªõc 3: Remove Emoji
        step3_text = VietnameseTextCleaner.remove_emoji(current_text)
        print_step_header(3, "LO·∫†I B·ªé EMOJI", "X√≥a c√°c emoji v√† emoticon")
        print_step_result(current_text, step3_text, "Remove Emoji")
        current_text = step3_text
        
        # B∆∞·ªõc 4: Remove URL
        step4_text = VietnameseTextCleaner.remove_url(current_text)
        print_step_header(4, "LO·∫†I B·ªé URL", "X√≥a c√°c li√™n k·∫øt web")
        print_step_result(current_text, step4_text, "Remove URL")
        current_text = step4_text
        
        # B∆∞·ªõc 5: Remove Email
        step5_text = VietnameseTextCleaner.remove_email(current_text)
        print_step_header(5, "LO·∫†I B·ªé EMAIL", "X√≥a ƒë·ªãa ch·ªâ email")
        print_step_result(current_text, step5_text, "Remove Email")
        current_text = step5_text
        
        # B∆∞·ªõc 6: Remove Phone
        step6_text = VietnameseTextCleaner.remove_phone_number(current_text)
        print_step_header(6, "LO·∫†I B·ªé S·ªê ƒêI·ªÜN THO·∫†I", "X√≥a s·ªë ƒëi·ªán tho·∫°i")
        print_step_result(current_text, step6_text, "Remove Phone")
        current_text = step6_text
        
        # B∆∞·ªõc 7: Remove Hashtags
        step7_text = VietnameseTextCleaner.remove_hashtags(current_text)
        print_step_header(7, "LO·∫†I B·ªé HASHTAGS", "X√≥a c√°c hashtag #tag")
        print_step_result(current_text, step7_text, "Remove Hashtags")
        current_text = step7_text
        
        # B∆∞·ªõc 8: Remove Unnecessary Characters
        step8_text = VietnameseTextCleaner.remove_unnecessary_characters(current_text)
        print_step_header(8, "LO·∫†I B·ªé K√ù T·ª∞ KH√îNG C·∫¶N THI·∫æT", "Ch·ªâ gi·ªØ ch·ªØ, s·ªë v√† d·∫•u ti·∫øng Vi·ªát")
        print_step_result(current_text, step8_text, "Clean Characters")
        current_text = step8_text
        
        # B∆∞·ªõc 9: Normalize Tone
        step9_text = normalize_tone_simple(current_text)
        print_step_header(9, "CHU·∫®N H√ìA TONE TI·∫æNG VI·ªÜT", "S·ª≠a l·ªói g√µ tone nh∆∞ l·ª±∆°ng -> l∆∞·ª£ng")
        print_step_result(current_text, step9_text, "Normalize Tone")
        current_text = step9_text
        
        # B∆∞·ªõc 10: Normalize Teencodes
        step10_text = normalize_teencodes_simple(current_text)
        print_step_header(10, "CHU·∫®N H√ìA TEENCODE", "Chuy·ªÉn teencode th√†nh t·ª´ chu·∫©n: ks -> kh√°ch s·∫°n")
        print_step_result(current_text, step10_text, "Normalize Teencode")
        current_text = step10_text
        
        # B∆∞·ªõc 11: Word Segmentation
        step11_text = segment_words_simple(current_text)
        print_step_header(11, "PH√ÇN ƒêO·∫†N T·ª™", "Gh√©p t·ª´ gh√©p: kh√°ch s·∫°n -> kh√°ch_s·∫°n")
        print_step_result(current_text, step11_text, "Word Segmentation")
        final_text = step11_text
        
        # T·ªïng k·∫øt
        print(f"\n{'üéâ'*10} K·∫æT QU·∫¢ CU·ªêI C√ôNG {'üéâ'*10}")
        print(f"üì• Input:  '{original_text}'")
        print(f"üì§ Output: '{final_text}'")
        
        # Th·ªëng k√™ thay ƒë·ªïi
        words_before = len(original_text.split())
        words_after = len(final_text.split())
        chars_before = len(original_text)
        chars_after = len(final_text)
        
        print(f"\nüìä Th·ªëng k√™:")
        print(f"   ‚Ä¢ S·ªë t·ª´: {words_before} ‚Üí {words_after} ({'+'*(words_after-words_before) if words_after >= words_before else str(words_after-words_before)})")
        print(f"   ‚Ä¢ S·ªë k√Ω t·ª±: {chars_before} ‚Üí {chars_after} ({'+'*(chars_after-chars_before) if chars_after >= chars_before else str(chars_after-chars_before)})")
        
        results.append({
            'sample_id': idx + 1,
            'original_text': original_text,
            'step1_lowercase': step1_text,
            'step2_remove_html': step2_text,
            'step3_remove_emoji': step3_text,
            'step4_remove_url': step4_text,
            'step5_remove_email': step5_text,
            'step6_remove_phone': step6_text,
            'step7_remove_hashtags': step7_text,
            'step8_clean_chars': step8_text,
            'step9_normalize_tone': step9_text,
            'step10_normalize_teencode': step10_text,
            'step11_word_segment': step11_text,
            'final_output': final_text,
            'words_before': words_before,
            'words_after': words_after,
            'chars_before': chars_before,
            'chars_after': chars_after
        })
    
    # Xu·∫•t k·∫øt qu·∫£ ra file CSV n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
    if output_file:
        df = pd.DataFrame(results)
        df.to_csv(output_file, index=False, encoding='utf-8')
        print(f"\nüíæ ƒê√£ l∆∞u k·∫øt qu·∫£ chi ti·∫øt v√†o: {output_file}")
    
    return results


def main():
    """Main function"""
    print("üáªüá≥ DEMO TI·ªÄN X·ª¨ L√ù VƒÇN B·∫¢N TI·∫æNG VI·ªÜT - ABSA VLSP 2018")
    print("=" * 80)
    
    # Sample texts cho demo
    sample_texts = [
        # Text v·ªõi HTML v√† emoji
        "Ks n√†y r·∫•t <b>ƒë·∫πp</b> v√† s·∫°ch s·∫Ω üòç! Nv th√¢n thi·ªán ok.",
        
        # Text v·ªõi URL v√† email
        "Nhahang t·ªët, xem th√™m t·∫°i https://example.com ho·∫∑c li√™n h·ªá test@gmail.com",
        
        # Text v·ªõi hashtags v√† phone
        "M√≥n ƒÉn ngon wa #food #delicious. G·ªçi 0901234567 ƒë·ªÉ ƒë·∫∑t b√†n.",
        
        # Text v·ªõi l·ªói tone v√† teencode
        "Ch·∫•t l·ª±∆°ng ko t·ªët, th·ªèai m√°i vs gi√° c·∫£ h·ª£p l√Ω. Tks!",
        
        # Text ph·ª©c t·∫°p v·ªõi nhi·ªÅu v·∫•n ƒë·ªÅ
        "C√°m ∆°n shop üòä! S·∫£n ph·∫©m gud, ship nhanh. Visit: www.shop.com #shopping üëç",
        
        # Text th·ª±c t·∫ø t·ª´ review kh√°ch s·∫°n
        "Ph√≤ng s·∫°ch s·∫Ω, nh√¢n vi√™n dv t·ªët. V·ªã tr√≠ kh√°ch s·∫°n thu·∫≠n ti·ªán, g·∫ßn trung t√¢m.",
        
        # Text v·ªõi nhi·ªÅu k√Ω t·ª± ƒë·∫∑c bi·ªát
        "!!! Gi√° c·∫£ ok, ch·∫•t l∆∞·ª£ng @#$% kh√¥ng nh∆∞ mong ƒë·ª£i !!! FB: hotelpage"
    ]
    
    # Ch·∫°y demo
    results = demo_preprocessing_pipeline(
        sample_texts, 
        output_file='preprocessing_demo_results.csv'
    )
    
    print(f"\n\n{'üî•'*20} T·ªîNG K·∫æT {'üî•'*20}")
    print(f"‚úÖ ƒê√£ x·ª≠ l√Ω {len(sample_texts)} sample texts")
    print(f"üìä K·∫øt qu·∫£ t·ªïng th·ªÉ:")
    
    total_chars_before = sum(r['chars_before'] for r in results)
    total_chars_after = sum(r['chars_after'] for r in results)
    total_words_before = sum(r['words_before'] for r in results)
    total_words_after = sum(r['words_after'] for r in results)
    
    print(f"   ‚Ä¢ T·ªïng k√Ω t·ª±: {total_chars_before} ‚Üí {total_chars_after}")
    print(f"   ‚Ä¢ T·ªïng t·ª´: {total_words_before} ‚Üí {total_words_after}")
    print(f"   ‚Ä¢ T·ª∑ l·ªá n√©n k√Ω t·ª±: {total_chars_after/total_chars_before:.2%}")
    print(f"   ‚Ä¢ T·ª∑ l·ªá thay ƒë·ªïi t·ª´: {total_words_after/total_words_before:.2%}")
    
    print(f"\nüéØ C√°c b∆∞·ªõc quan tr·ªçng nh·∫•t:")
    print(f"   1. Chu·∫©n h√≥a teencode (ks ‚Üí kh√°ch s·∫°n)")
    print(f"   2. Lo·∫°i b·ªè k√Ω t·ª± kh√¥ng c·∫ßn thi·∫øt")
    print(f"   3. Ph√¢n ƒëo·∫°n t·ª´ (kh√°ch s·∫°n ‚Üí kh√°ch_s·∫°n)")
    print(f"   4. Chu·∫©n h√≥a tone ti·∫øng Vi·ªát")
    
    print(f"\nüìù L∆∞u √Ω: Script n√†y s·ª≠ d·ª•ng preprocessing ƒë∆°n gi·∫£n.")
    print(f"   Trong th·ª±c t·∫ø, project s·ª≠ d·ª•ng:")
    print(f"   ‚Ä¢ VnCoreNLP cho word segmentation ch√≠nh x√°c")
    print(f"   ‚Ä¢ Model bmd1905/vietnamese-correction-v2 cho error correction")
    print(f"   ‚Ä¢ Teencode dictionary ƒë·∫ßy ƒë·ªß t·ª´ behitek")


if __name__ == "__main__":
    main() 